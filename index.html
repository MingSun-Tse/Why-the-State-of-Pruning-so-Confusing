<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="On-Fairness-Trainability-in-Network-Pruning">
  <meta name="keywords" content="Knowledge Distillation, Data Augmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Is that Pruning Experiment Really Fair? On the Central Role of Trainability in Neural Network Pruning</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://github.com/MingSun-Tse/On-Fairness-Trainability-in-Network-Pruning">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Is that Pruning Experiment Really Fair? On the Central Role of Trainability in Neural Network Pruning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="http://huanwang.tech">Huan Wang</a><sup>1,&dagger;</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="https://canqin.tech/">Can Qin</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="https://yueb17.github.io/">Yue Bai</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a><sup>1</sup>
              </span>
            </div>
            <h1 style="font-size:23px;font-weight:bold">Preprint, 2022</h1>

            <div class="is-size-5 publication-authors">
              <span><sup>1</sup>Northeastern University, Boston, MA&#8192;</span>
              <span><sup>&dagger;</sup>Corresponding author: wang [dot] huan [at] northeastern [dot] edu &#8192;<sup></span></br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2105.05916.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>PDF</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2105.05916" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/MingSun-Tse/Smile-Pruning"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (will update soon)</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div align='center'>
          <a><img src="figs/frontpage_overview.svg"  height="200" ></a>
          <a><img src="figs/wrn_40_2_wrn_16_2_cifar100.svg"  height="200" ></a>
        </div>
        <div class="content has-text-justified">
          Network pruning, despite over 30 years of history, has been pretty confounding of late in terms of its benchmarks. As quoted from this survey paper: ""
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            Knowledge distillation (KD) is a general neural network training approach that uses a teacher to guide a student. Existing works mainly study KD from the network output side (e.g., trying to design a better KD loss function), while few have attempted to understand it from the input side. Especially, its interplay with data augmentation (DA) has not been well understood. In this paper, we ask: Why do some DA schemes (e.g., CutMix) inherently perform much better than others in KD? What makes a “good” DA in KD? Our investigation from a statistical perspective suggests that a good DA scheme should reduce the variance of the teacher’s mean probability, which will eventually lead to a lower generalization gap for the student. Besides the theoretical understanding, we also introduce a new entropy-based data-mixing DA scheme to enhance CutMix. Extensive empirical studies support our claims and demonstrate how we can harvest considerable performance gains simply by using a better DA scheme in knowledge distillation.
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">

          <!-- NeRF results: Blender -->
          <h2 class="title is-3" align='center'>Correlation Results on CIFAR100 & Tiny ImageNet</h2>

          <p align="center">
            <a><img src="figs/wrn_40_2_wrn_16_2_cifar100.svg"  height="200" ></a>
            <a><img src="figs/resnet56_resnet20_cifar100.svg"  height="200" ></a>
            </br>
            <a><img src="figs/vgg13_vgg8_cifar100.svg"  height="200" ></a>
            <a><img src="figs/ResNet50_vgg8_cifar100.svg"  height="200" ></a>
            </br>
            <a><img src="figs/wrn_40_2_wrn_16_2_tinyimagenet.svg"  height="200" ></a>
            <a><img src="figs/resnet56_resnet20_tinyimagenet.svg"  height="200" ></a>
            </br>
            <a><img src="figs/vgg13_vgg8_tinyimagenet.svg"  height="200" ></a>
            <a><img src="figs/ResNet50_vgg8_tinyimagenet.svg"  height="200" ></a>
            </br>
          </p>

          <h2 class="title is-3" align='center'>Correlation Results on ImageNet100</h2>
          ImageNet100 is a 100-class subset randomly drawn from the full ImageNet-1K dataset. The correlation turns weaker (compared to CIFAR100 and Tiny ImageNet) generally speaking, due to the fact that ImageNet100 is essentially more challenging than CIFAR100 and Tiny ImageNet. But still, suggested by the p-values, the correlation is fairly strong. This means the effectiveness of our metric can generalize to the standard 224x224 RGB images.

          <p align="center">
            <a><img src="figs/resnet34_resnet18_imagenet100.svg"  height="200" ></a>
          </p>

          <h2 class="title is-3" align='center'>Correlation Results on ImageNet</h2>
          The results on ImageNet are not very much aligned with our expectation, as the correlation between S. test loss and T. stddev below is not statistically significant. We don't know why for now (presumbly think it might be related to the number of classes of the dataset). We consider this as a limitation of this work and shall explore it in the future version.
          <p align="center">
            <a><img src="figs/resnet34_resnet18_imagenet.svg"  height="200" ></a>
          </p>

          <h2 class="title is-3" align='center'>Boosting KD via Stronger KD + More Epochs </br>(CIFAR100 and Tiny ImageNet)</h2>
          As shown below, compared to the original KD results, we can harvest considerable performance gains simply by using a stronger DA (CutMix and our proposed CutMixPick) with more training epochs.
          <br /> <br />
          <p align="center">
            <a><img src="figs/boosting_kd.svg"  width="700" ></a>
          </p>
        </div>
      </div>
    </div>
  </section>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wang2022what,
          author = {Huan Wang and Can Qin and Yue Bai and Yun Fu},
          title = {Is that Pruning Experiment Really Fair? On the Central Role of Trainability in Neural Network Pruning},
          journal = {arXiv preprint arXiv:2105.05916},
          year = {2022},
        }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div align="center" class="container">
      <div class="columns is-centered">
        <div class="content">
          This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </footer>

</body>

</html>